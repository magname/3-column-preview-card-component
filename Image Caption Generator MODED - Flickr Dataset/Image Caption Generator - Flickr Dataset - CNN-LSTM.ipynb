{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4_aVJPk1kIX"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:34:14.990195Z",
          "iopub.status.busy": "2022-03-06T06:34:14.989864Z",
          "iopub.status.idle": "2022-03-06T06:34:19.970661Z",
          "shell.execute_reply": "2022-03-06T06:34:19.969956Z",
          "shell.execute_reply.started": "2022-03-06T06:34:14.990114Z"
        },
        "id": "1l6qs-rT1kIc"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from math import ceil\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm        # Progress bar library for Jupyter Notebook\n",
        "\n",
        "# Deep learning framework for building and training models\n",
        "import tensorflow as tf\n",
        "## Pre-trained model for image feature extraction\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "## Tokenizer class for captions tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "## Function for padding sequences to a specific length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "## Class for defining Keras models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, Bidirectional, Dot, Activation, RepeatVector, Multiply, Lambda\n",
        "\n",
        "# For checking score\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:34:19.972539Z",
          "iopub.status.busy": "2022-03-06T06:34:19.972303Z",
          "iopub.status.idle": "2022-03-06T06:34:19.978812Z",
          "shell.execute_reply": "2022-03-06T06:34:19.977587Z",
          "shell.execute_reply.started": "2022-03-06T06:34:19.972501Z"
        },
        "id": "YmvNU2NX1kIh"
      },
      "outputs": [],
      "source": [
        "INPUT_DIR = '/kaggle/input/flickr8k'\n",
        "OUTPUT_DIR = '/kaggle/working'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG-b-9so1kIk"
      },
      "source": [
        "## Extract Image Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:34:19.982304Z",
          "iopub.status.busy": "2022-03-06T06:34:19.982057Z",
          "iopub.status.idle": "2022-03-06T06:34:26.712594Z",
          "shell.execute_reply": "2022-03-06T06:34:26.711913Z",
          "shell.execute_reply.started": "2022-03-06T06:34:19.982271Z"
        },
        "id": "wCuy57Fd1kIl",
        "outputId": "ef931223-13f5-4026-ae8d-b64abe52e5d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-06 06:34:20.067223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:20.193909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:20.194628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:20.195695: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-03-06 06:34:20.196381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:20.197047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:20.197765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:22.079249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:22.080322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:22.081336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-03-06 06:34:22.082254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n",
            "553476096/553467096 [==============================] - 3s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# We are going to use pretraind vgg model\n",
        "# Load the vgg16 model\n",
        "model = VGG16()\n",
        "\n",
        "# Restructuring the model to remove the last classification layer, this will give us access to the output features of the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "# Printing the model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:34:26.714171Z",
          "iopub.status.busy": "2022-03-06T06:34:26.713924Z",
          "iopub.status.idle": "2022-03-06T06:42:34.816462Z",
          "shell.execute_reply": "2022-03-06T06:42:34.815809Z",
          "shell.execute_reply.started": "2022-03-06T06:34:26.714138Z"
        },
        "id": "n05RCoqL1kIo",
        "outputId": "3029ea75-49ea-4d6d-8c5f-4d5b5d1338ff",
        "colab": {
          "referenced_widgets": [
            "007700728ac24b9d8f1e64f6530d914e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "007700728ac24b9d8f1e64f6530d914e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8091 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-06 06:34:27.116746: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2022-03-06 06:34:27.997787: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty dictionary to store image features\n",
        "image_features = {}\n",
        "\n",
        "# Define the directory path where images are located\n",
        "img_dir = os.path.join(INPUT_DIR, 'Images')\n",
        "\n",
        "# Loop through each image in the directory\n",
        "for img_name in tqdm(os.listdir(img_dir)):\n",
        "    # Load the image from file\n",
        "    img_path = os.path.join(img_dir, img_name)\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # Convert image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # Reshape the data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # Preprocess the image for ResNet50\n",
        "    image = preprocess_input(image)\n",
        "    # Extract features using the pre-trained ResNet50 model\n",
        "    image_feature = model.predict(image, verbose=0)\n",
        "    # Get the image ID by removing the file extension\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # Store the extracted feature in the dictionary with the image ID as the key\n",
        "    image_features[image_id] = image_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:34.819143Z",
          "iopub.status.busy": "2022-03-06T06:42:34.818823Z",
          "iopub.status.idle": "2022-03-06T06:42:35.148181Z",
          "shell.execute_reply": "2022-03-06T06:42:35.147422Z",
          "shell.execute_reply.started": "2022-03-06T06:42:34.819101Z"
        },
        "id": "bTvE2fIe1kIr"
      },
      "outputs": [],
      "source": [
        "# store features in pickle\n",
        "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.150092Z",
          "iopub.status.busy": "2022-03-06T06:42:35.149415Z",
          "iopub.status.idle": "2022-03-06T06:42:35.312615Z",
          "shell.execute_reply": "2022-03-06T06:42:35.311884Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.150052Z"
        },
        "id": "FNznw_Ym1kIt"
      },
      "outputs": [],
      "source": [
        "# Load features from pickle file\n",
        "pickle_file_path = os.path.join(OUTPUT_DIR, 'img_features.pkl')\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    loaded_features = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOGrgWlh1kIw"
      },
      "source": [
        "## Load the Captions Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.315520Z",
          "iopub.status.busy": "2022-03-06T06:42:35.314779Z",
          "iopub.status.idle": "2022-03-06T06:42:35.378135Z",
          "shell.execute_reply": "2022-03-06T06:42:35.377462Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.315478Z"
        },
        "id": "EugiLnpZ1kIx"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(INPUT_DIR, 'captions.txt'), 'r') as file:\n",
        "    next(file)\n",
        "    captions_doc = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.379822Z",
          "iopub.status.busy": "2022-03-06T06:42:35.379546Z",
          "iopub.status.idle": "2022-03-06T06:42:35.520768Z",
          "shell.execute_reply": "2022-03-06T06:42:35.520076Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.379783Z"
        },
        "id": "VHJAfLbi1kIz",
        "outputId": "15296366-a205-49a6-f8c5-ae2519ff8e66",
        "colab": {
          "referenced_widgets": [
            "b025825478b14b48809fd28a00b9d4f0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b025825478b14b48809fd28a00b9d4f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40456 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create mapping of image to captions\n",
        "image_to_captions_mapping = defaultdict(list)\n",
        "\n",
        "# Process lines from captions_doc\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # Split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "    if len(tokens) < 2:\n",
        "        continue\n",
        "    image_id, *captions = tokens\n",
        "    # Remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # Convert captions list to string\n",
        "    caption = \" \".join(captions)\n",
        "    # Store the caption using defaultdict\n",
        "    image_to_captions_mapping[image_id].append(caption)\n",
        "\n",
        "# Print the total number of captions\n",
        "total_captions = sum(len(captions) for captions in image_to_captions_mapping.values())\n",
        "print(\"Total number of captions:\", total_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmsoqeAo1kI1"
      },
      "source": [
        "## Preprocess Text Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NN8MqBo-5eoG",
        "outputId": "fe1bd225-3811-45a4-dd78-3c1f00638380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.531789Z",
          "iopub.status.busy": "2022-03-06T06:42:35.531417Z",
          "iopub.status.idle": "2022-03-06T06:42:35.539936Z",
          "shell.execute_reply": "2022-03-06T06:42:35.539143Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.531753Z"
        },
        "id": "Sl_IP5rl1kI2"
      },
      "outputs": [],
      "source": [
        "# Function for processing the captions\n",
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # Take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # Preprocessing steps\n",
        "            # Convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # Remove non-alphabetical characters\n",
        "            caption = ''.join(char for char in caption if char.isalpha() or char.isspace())\n",
        "            # Remove extra spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # Add unique start and end tokens to the caption\n",
        "            caption = 'startseq ' + ' '.join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.541972Z",
          "iopub.status.busy": "2022-03-06T06:42:35.541482Z",
          "iopub.status.idle": "2022-03-06T06:42:35.550972Z",
          "shell.execute_reply": "2022-03-06T06:42:35.549978Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.541937Z"
        },
        "id": "3n78ko4Z1kI3",
        "outputId": "a4958d5f-78db-494b-c7f7-d7002c5531d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# before preprocess of text\n",
        "image_to_captions_mapping['1026685415_0431cbf574']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text\n",
        "clean(image_to_captions_mapping)"
      ],
      "metadata": {
        "id": "d3A2Chfp3vdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after preprocess of text\n",
        "image_to_captions_mapping['1026685415_0431cbf574']"
      ],
      "metadata": {
        "id": "KjHKa0jP3wq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.553409Z",
          "iopub.status.busy": "2022-03-06T06:42:35.552825Z",
          "iopub.status.idle": "2022-03-06T06:42:35.692145Z",
          "shell.execute_reply": "2022-03-06T06:42:35.691520Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.553368Z"
        },
        "id": "LvKgynWw1kI4"
      },
      "outputs": [],
      "source": [
        "# Creating a List of All Captions\n",
        "all_captions = [caption for captions in image_to_captions_mapping.values() for caption in captions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.693654Z",
          "iopub.status.busy": "2022-03-06T06:42:35.693412Z",
          "iopub.status.idle": "2022-03-06T06:42:35.698691Z",
          "shell.execute_reply": "2022-03-06T06:42:35.697951Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.693623Z"
        },
        "id": "oIYOmsoe1kI4",
        "outputId": "649ce7a2-97a9-424b-d609-982cd11b5dce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
              " 'startseq girl going into wooden building endseq',\n",
              " 'startseq little girl climbing into wooden playhouse endseq',\n",
              " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
              " 'startseq little girl in pink dress going into wooden cabin endseq']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_captions[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.703681Z",
          "iopub.status.busy": "2022-03-06T06:42:35.703176Z",
          "iopub.status.idle": "2022-03-06T06:42:35.716917Z",
          "shell.execute_reply": "2022-03-06T06:42:35.716311Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.703644Z"
        },
        "id": "m1ojWYyT1kI5"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the Text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.719878Z",
          "iopub.status.busy": "2022-03-06T06:42:35.719667Z",
          "iopub.status.idle": "2022-03-06T06:42:35.728236Z",
          "shell.execute_reply": "2022-03-06T06:42:35.727288Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.719843Z"
        },
        "id": "JHemViNq1kI5",
        "outputId": "f88ebece-43ed-410a-c1ca-e3fa3f8792d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
        "    pickle.dump(tokenizer, tokenizer_file)\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
        "    tokenizer = pickle.load(tokenizer_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate maximum caption length\n",
        "max_caption_length = max(len(tokenizer.texts_to_sequences([caption])[0]) for caption in all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Print the results\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "print(\"Maximum Caption Length:\", max_caption_length)"
      ],
      "metadata": {
        "id": "ZlmYH06535Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.730327Z",
          "iopub.status.busy": "2022-03-06T06:42:35.730058Z",
          "iopub.status.idle": "2022-03-06T06:42:35.737755Z",
          "shell.execute_reply": "2022-03-06T06:42:35.736976Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.730295Z"
        },
        "id": "zatfoPmH1kI6",
        "outputId": "b1f1c3d0-a70e-4726-8685-96cf1a876967"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
              " 'startseq girl going into wooden building endseq',\n",
              " 'startseq little girl climbing into wooden playhouse endseq',\n",
              " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
              " 'startseq little girl in pink dress going into wooden cabin endseq',\n",
              " 'startseq black dog and spotted dog are fighting endseq',\n",
              " 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n",
              " 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n",
              " 'startseq two dogs of different breeds looking at each other on the road endseq',\n",
              " 'startseq two dogs on pavement moving toward each other endseq']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a List of Image IDs\n",
        "image_ids = list(image_to_captions_mapping.keys())\n",
        "# Splitting into Training and Test Sets\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "id": "fmN7ole537jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generator function\n",
        "def data_generator(data_keys, image_to_captions_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):\n",
        "    # Lists to store batch data\n",
        "    X1_batch, X2_batch, y_batch = [], [], []\n",
        "    # Counter for the current batch size\n",
        "    batch_count = 0\n",
        "\n",
        "    while True:\n",
        "        # Loop through each image in the current batch\n",
        "        for image_id in data_keys:\n",
        "            # Get the captions associated with the current image\n",
        "            captions = image_to_captions_mapping[image_id]\n",
        "\n",
        "            # Loop through each caption for the current image\n",
        "            for caption in captions:\n",
        "                # Convert the caption to a sequence of token IDs\n",
        "                caption_seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "                # Loop through the tokens in the caption sequence\n",
        "                for i in range(1, len(caption_seq)):\n",
        "                    # Split the sequence into input and output pairs\n",
        "                    in_seq, out_seq = caption_seq[:i], caption_seq[i]\n",
        "\n",
        "                    # Pad the input sequence to the specified maximum caption length\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
        "\n",
        "                    # Convert the output sequence to one-hot encoded format\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                    # Append data to batch lists\n",
        "                    X1_batch.append(features[image_id][0])  # Image features\n",
        "                    X2_batch.append(in_seq)  # Input sequence\n",
        "                    y_batch.append(out_seq)  # Output sequence\n",
        "\n",
        "                    # Increase the batch counter\n",
        "                    batch_count += 1\n",
        "\n",
        "                    # If the batch is complete, yield the batch and reset lists and counter\n",
        "                    if batch_count == batch_size:\n",
        "                        X1_batch, X2_batch, y_batch = np.array(X1_batch), np.array(X2_batch), np.array(y_batch)\n",
        "                        yield [X1_batch, X2_batch], y_batch\n",
        "                        X1_batch, X2_batch, y_batch = [], [], []\n",
        "                        batch_count = 0"
      ],
      "metadata": {
        "id": "OzgML2Rk3-jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:35.739724Z",
          "iopub.status.busy": "2022-03-06T06:42:35.739327Z",
          "iopub.status.idle": "2022-03-06T06:42:36.355776Z",
          "shell.execute_reply": "2022-03-06T06:42:36.355062Z",
          "shell.execute_reply.started": "2022-03-06T06:42:35.739686Z"
        },
        "id": "wBRs2Enb1kI7"
      },
      "outputs": [],
      "source": [
        "# Encoder model\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "fe2_projected = RepeatVector(max_caption_length)(fe2)\n",
        "fe2_projected = Bidirectional(LSTM(256, return_sequences=True))(fe2_projected)\n",
        "\n",
        "# Sequence feature layers\n",
        "inputs2 = Input(shape=(max_caption_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)\n",
        "\n",
        "# Apply attention mechanism using Dot product\n",
        "attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores\n",
        "\n",
        "# Softmax attention scores\n",
        "attention_scores = Activation('softmax')(attention)\n",
        "\n",
        "# Apply attention scores to sequence embeddings\n",
        "attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])\n",
        "\n",
        "# Sum the attended sequence embeddings along the time axis\n",
        "context_vector = tf.reduce_sum(attention_context, axis=1)\n",
        "\n",
        "# Decoder model\n",
        "decoder_input = concatenate([context_vector, fe2], axis=-1)\n",
        "decoder1 = Dense(256, activation='relu')(decoder_input)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Visualize the model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:36.357418Z",
          "iopub.status.busy": "2022-03-06T06:42:36.357182Z",
          "iopub.status.idle": "2022-03-06T06:42:36.362429Z",
          "shell.execute_reply": "2022-03-06T06:42:36.361793Z",
          "shell.execute_reply.started": "2022-03-06T06:42:36.357385Z"
        },
        "id": "g06rNpBh1kI7",
        "outputId": "7ba4b674-9a99-4f95-a062-e0810a5f42c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8485"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the number of epochs, batch size\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "# Calculate the steps_per_epoch based on the number of batches in one epoch\n",
        "steps_per_epoch = ceil(len(train) / batch_size)\n",
        "validation_steps = ceil(len(test) / batch_size)  # Calculate the steps for validation data\n",
        "\n",
        "# Loop through the epochs for training\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Set up data generators\n",
        "    train_generator = data_generator(train, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
        "    test_generator = data_generator(test, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
        "\n",
        "    model.fit(train_generator, epochs=1, steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=test_generator, validation_steps=validation_steps,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:36.364347Z",
          "iopub.status.busy": "2022-03-06T06:42:36.363844Z",
          "iopub.status.idle": "2022-03-06T06:42:36.403655Z",
          "shell.execute_reply": "2022-03-06T06:42:36.402785Z",
          "shell.execute_reply.started": "2022-03-06T06:42:36.364309Z"
        },
        "id": "VHqM_3-z1kI8",
        "outputId": "9036f673-afd8-428c-f9ca-a7eb5c296473"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(OUTPUT_DIR+'/mymodel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HX75z59O4SfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_from_index(index, tokenizer):\n",
        "    return next((word for word, idx in tokenizer.word_index.items() if idx == index), None)"
      ],
      "metadata": {
        "id": "p7Aods-74Q6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(model, image_features, tokenizer, max_caption_length):\n",
        "    # Initialize the caption sequence\n",
        "    caption = 'startseq'\n",
        "\n",
        "    # Generate the caption\n",
        "    for _ in range(max_caption_length):\n",
        "        # Convert the current caption to a sequence of token indices\n",
        "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "        # Pad the sequence to match the maximum caption length\n",
        "        sequence = pad_sequences([sequence], maxlen=max_caption_length)\n",
        "        # Predict the next word's probability distribution\n",
        "        yhat = model.predict([image_features, sequence], verbose=0)\n",
        "        # Get the index with the highest probability\n",
        "        predicted_index = np.argmax(yhat)\n",
        "        # Convert the index to a word\n",
        "        predicted_word = get_word_from_index(predicted_index, tokenizer)\n",
        "\n",
        "        # Append the predicted word to the caption\n",
        "        caption += \" \" + predicted_word\n",
        "\n",
        "        # Stop if the word is None or if the end sequence tag is encountered\n",
        "        if predicted_word is None or predicted_word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return caption"
      ],
      "metadata": {
        "id": "zomPbMH44Y_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:36.406025Z",
          "iopub.status.busy": "2022-03-06T06:42:36.405019Z",
          "iopub.status.idle": "2022-03-06T06:42:36.411229Z",
          "shell.execute_reply": "2022-03-06T06:42:36.410251Z",
          "shell.execute_reply.started": "2022-03-06T06:42:36.405987Z"
        },
        "id": "FVWhJ29w1kI9"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store actual and predicted captions\n",
        "actual_captions_list = []\n",
        "predicted_captions_list = []\n",
        "\n",
        "# Loop through the test data\n",
        "for key in tqdm(test):\n",
        "    # Get actual captions for the current image\n",
        "    actual_captions = image_to_captions_mapping[key]\n",
        "    # Predict the caption for the image using the model\n",
        "    predicted_caption = predict_caption(model, loaded_features[key], tokenizer, max_caption_length)\n",
        "\n",
        "    # Split actual captions into words\n",
        "    actual_captions_words = [caption.split() for caption in actual_captions]\n",
        "    # Split predicted caption into words\n",
        "    predicted_caption_words = predicted_caption.split()\n",
        "\n",
        "    # Append to the lists\n",
        "    actual_captions_list.append(actual_captions_words)\n",
        "    predicted_captions_list.append(predicted_caption_words)\n",
        "\n",
        "# Calculate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual_captions_list, predicted_captions_list, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual_captions_list, predicted_captions_list, weights=(0.5, 0.5, 0, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T06:42:36.413094Z",
          "iopub.status.busy": "2022-03-06T06:42:36.412818Z",
          "iopub.status.idle": "2022-03-06T06:42:36.420669Z",
          "shell.execute_reply": "2022-03-06T06:42:36.419873Z",
          "shell.execute_reply.started": "2022-03-06T06:42:36.413060Z"
        },
        "id": "CWqcxQEk1kI-"
      },
      "outputs": [],
      "source": [
        "# Function for generating caption\n",
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(INPUT_DIR, \"Images\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = image_to_captions_mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, loaded_features[image_id], tokenizer, max_caption_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-9g4pvm1kJd"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"101669240_b2d3e7f17b.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsFJeNB31kJd"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"1077546505_a4f6c4daa9.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1002674143_1b742ab4b8.jpg\")"
      ],
      "metadata": {
        "id": "PTPKymHL4qDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1032460886_4a598ed535.jpg\")"
      ],
      "metadata": {
        "id": "Nau4be5p4rcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1032122270_ea6f0beedb.jpg\")"
      ],
      "metadata": {
        "id": "WVyxFy1W4sns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}